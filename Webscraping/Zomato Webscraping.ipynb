{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members:\n",
    "\n",
    "Saurabh Sisodia (PGID: 11915035)\n",
    "\n",
    "Sivapradeep Katiki (PGID: 11915024)\n",
    "\n",
    "Vikaskashyap Konda (PGID: 11915047)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zomato Webscraping Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from math import ceil\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Initialize the Selenium Webdriver for scraping the data from zomato website.\n",
    "- Navigate to \"https://www.zomato.com/Hyderabad\"\n",
    "- Search by keyword 'Gachibowli, Hyderabad'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome('C:/Users/xyz/Documents/ISBCBA/Term1/Data Collection/LEC 2-2019-04-12/LEC 2/data/chromedriver.exe')\n",
    "driver = webdriver.Firefox(executable_path=r'C:/Users/xyz/Documents/ISBCBA/Term1/Data Collection/geckodriver-v0.24.0-win64/geckodriver.exe')\n",
    "driver.get(\"https://www.zomato.com/Hyderabad\")\n",
    "assert \"Zomato\" in driver.title\n",
    "elem = driver.find_element_by_css_selector('#keywords_input')\n",
    "elem.clear()\n",
    "elem.send_keys(\"Gachibowli, Hyderabad\")\n",
    "time.sleep(5)\n",
    "elem.send_keys(Keys.RETURN)  \n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the restaurant dataframe to store restaurant data from zomato website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant = pd.DataFrame(columns=['Name','Link','Timing','Rating','Vote','Cuisine','Cost','Collection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrape the zomato website and collect the restaurant information\n",
    "\n",
    "We will be scraping first 100 restaurants in Gachibowli area.\n",
    "\n",
    "- Capture basic information of a restaurant\n",
    "    - Restaurant Name\n",
    "    - Restaurant Link\n",
    "    - Restaurant Timing\n",
    "    - Restaurant Vote\n",
    "    - Restaurant Cuisine\n",
    "    - Restaurant Cost\n",
    "    - Restaurant Collections\n",
    "- Save the information of each restaurant in a Dataframe 'df_restaurant'\n",
    "- Navigate through first 10 pages by clicking on next page button (as we want to capture details for first 100 restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "###Get Restaurant details######################################################\n",
    "###############################################################################\n",
    "for page in range(1,11):\n",
    "    time.sleep(5)\n",
    "    print('------loading page'+str(page)+'-----------')\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html,features=\"lxml\")\n",
    "    ###Capturing restaurant details\n",
    "    for i in soup.find_all('article',class_='search-result'):\n",
    "        restaurant_dict={}\n",
    "        try:\n",
    "            Name = str(i.find('a',class_='result-title').get('title')).replace(', Gachibowli','')\n",
    "        except:\n",
    "            Name=''\n",
    "        #Getting the link of the resturant to capture user reviews from the restaurant page later.\n",
    "        try:\n",
    "            Link = i.find('a',class_='result-title').get('href')\n",
    "        except:\n",
    "            Link=''\n",
    "        try:\n",
    "            Timing = i.find('div',class_='res-timing clearfix').get('title')\n",
    "        except:\n",
    "            Timing=''\n",
    "        try:\n",
    "            Rating = i.find('div',class_='res-rating-nf').string.strip()\n",
    "        except:\n",
    "            Rating=''\n",
    "        try:\n",
    "            Vote = i.find('span',class_=re.compile(\"rating-votes-div-\")).string.strip()\n",
    "        except:\n",
    "            Vote = '0'\n",
    "        #There are multiple cuisine for a restaurant, hence iterating over list to get all cuisines\n",
    "        try:\n",
    "            Cuisine=''\n",
    "            for j in i.find_all('span',class_='col-s-11 col-m-12 nowrap pl0'):\n",
    "                for k in j.find_all('a'):\n",
    "                    Cuisine = k.string + ', ' + Cuisine\n",
    "            Cuisine = Cuisine.strip()\n",
    "            Cuisine = Cuisine[:len(Cuisine)-1]\n",
    "        except:\n",
    "            Cuisine=''\n",
    "        try:\n",
    "            Cost = i.find('span',class_='col-s-11 col-m-12 pl0').string.strip()\n",
    "        except:\n",
    "            Cost=''\n",
    "        #Restaurant can be tagged under multiple collections, hence iterating over the list to get all collection tags\n",
    "        try:\n",
    "            Collection=''\n",
    "            for j in i.find_all('div',class_='res-collections clearfix'):\n",
    "                for k in j.find_all('a'):\n",
    "                    Collection = Collection + ', ' + k.string\n",
    "            Collection = Collection[2:]\n",
    "        except:\n",
    "            Collection=''\n",
    "            \n",
    "        restaurant_dict['Name'] = Name\n",
    "        restaurant_dict['Link'] = Link\n",
    "        restaurant_dict['Rating'] = Rating\n",
    "        restaurant_dict['Vote'] = Vote\n",
    "        restaurant_dict['Cuisine'] = Cuisine\n",
    "        restaurant_dict['Cost'] = Cost\n",
    "        restaurant_dict['Collection'] = Collection\n",
    "        restaurant_dict['Timing'] = Timing\n",
    "        df_restaurant = df_restaurant.append(restaurant_dict,ignore_index=True)\n",
    "    if(page==1):\n",
    "        driver.find_element_by_xpath('//*[@id=\"search-results-container\"]/div[2]/div[1]/div[2]/div/div/a[7]').click()\n",
    "    elif(page<=5):\n",
    "        driver.find_element_by_xpath('//*[@id=\"search-results-container\"]/div[2]/div[1]/div[2]/div/div/a[8]').click()\n",
    "    else:\n",
    "        driver.find_element_by_xpath('//*[@id=\"search-results-container\"]/div[2]/div[1]/div[2]/div/div/a[9]').click()\n",
    "    time.sleep(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the current working directory\n",
    "- Change the working directory to desired location\n",
    "- Create a new csv to write the information with following columns\n",
    "    - Restaurant Name\n",
    "    - Cuisine\n",
    "    - Cost\n",
    "    - Timing\n",
    "    - Rating\n",
    "    - Vote\n",
    "    - Collection\n",
    "    - Reviewer Name\n",
    "    - Review Text\n",
    "    - Review Time\n",
    "    - Total number of reviews given by reviewer\n",
    "    - Number of followers of reviewer\n",
    "    - Number of photos added by reviewer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a csv file at the given location in order to capture and save the restaurant details in a CSV file.\n",
    "os.getcwd()\n",
    "os.chdir(\"C:\\\\Users\\\\xyz\\\\Documents\\\\ISBCBA\\\\Term1\\\\Data Collection\\\\MyPractice\")\n",
    "os.listdir()\n",
    "csv_file = open(\"zomato_25May_final.csv\",'w',encoding ='UTF-8',newline = \"\")  \n",
    "writer = csv.writer(csv_file)\n",
    "writer.writerow(\n",
    "    ['Restaurant_Name','Cuisine','Cost','Restaurant_Timing','Restaurant_Rating','Vote','Collection',\n",
    "     'Reviewer_Name','Review_text','Review_time','NoOfReview','NoOfFollower','NoOfPhoto'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to get first 100 reviews provided by the users for tha restaurant\n",
    "\n",
    "- The function navigates to the restaurant page via selenium webdriver\n",
    "- Checks the total number of reviews for the restaurant\n",
    "- Click on to get 'All reviews' for the restaurant\n",
    "- The function scrapes for first 100 reviews (from latest to older ones)\n",
    "- The function picks up follwing information:\n",
    "    - Reviewer Name\n",
    "    - Reviewer Text : Some times the review text is large, so the function checks for 'Read More' button, to load the entire review text before scraping the data\n",
    "    - Reviewer Follower : Number of followers of the reviewer\n",
    "    - Reviewer Reviews : Total number of reviews provided by the user(reviewer)\n",
    "    - Reviewer Photos : Number of photos added by the reviewer\n",
    "    - Reviewer Time : Timestamp when the review was added by the reviewer\n",
    "- Function saves the information in the dictionary\n",
    "- Appends the dictionary to the the 'CSV' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def GetReviews(Link):\n",
    "    driver.get(Link)\n",
    "    time.sleep(10)\n",
    "    Review_counts = int(driver.find_element_by_xpath('//*[@id=\"selectors\"]/a[2]/span').text)\n",
    "    print(Review_counts)\n",
    "    #wait for 10 sec until element is clickable\n",
    "    WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH,'.//a[@data-sort=\"reviews-dd\"]'))).click()\n",
    "    time.sleep(5)\n",
    "    #If review count is more than 101 limit the scraping reviews to 100\n",
    "    if(int(Review_counts) > 101):\n",
    "        Review_counts = 101\n",
    "    for i in range(1,Review_counts):\n",
    "        try:\n",
    "            # Creating an empty dictionary\n",
    "            dict1={}\n",
    "            # Saving restaurant details from the restaurant dataframe to dictionary \n",
    "            dict1['Restaurant_Name'] = df_restaurant['Name'][j]\n",
    "            dict1['Cuisine'] = df_restaurant['Cuisine'][j]\n",
    "            dict1['Cost'] = df_restaurant['Cost'][j][1:] #ignoring Rupee currency#\n",
    "            dict1['Restaurant_Timing'] = df_restaurant['Timing'][j]\n",
    "            dict1['Restaurant_Rating'] = df_restaurant['Rating'][j]\n",
    "            dict1['Vote'] = df_restaurant['Vote'][j]\n",
    "            dict1['Collection'] = df_restaurant['Collection'][j]\n",
    "            print('#############'+str(i)+'##################')\n",
    "            \n",
    "            # Check to click on 'Load more' button to load more reviews\n",
    "            if(i%5==0):\n",
    "                print('x')\n",
    "                driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[2]/div[1]/span[1]').click()\n",
    "                time.sleep(10)\n",
    "            #Capture the details from review container block \n",
    "            review = driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div['+str(i)+']/div')\n",
    "            try:\n",
    "                print(review.find_element_by_xpath('.//div[@class = \"header nowrap ui left\"]').text)\n",
    "                rev_name = review.find_element_by_xpath('.//div[@class = \"header nowrap ui left\"]').text\n",
    "            except:\n",
    "                rev_name = ''\n",
    "            # Xpath for the 1st review comment is different from all other review comments,\n",
    "            # Hence a check if it is 1st review comment or not\n",
    "            # For some reviews the review text is large, hence need to check for 'read more' option in order to \n",
    "            # load full review text\n",
    "            try:\n",
    "                if(i==1):\n",
    "                    if(driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div[1]/div/div[1]/div[4]').text == ''):\n",
    "                        print('***********1********')\n",
    "                        if(driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div[1]/div/div[1]/div[5]/div/a').text == 'read more'):\n",
    "                            url='//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div[1]/div/div[1]/div[5]/div/a'\n",
    "                            driver.find_element_by_xpath(url).click()\n",
    "                            time.sleep(5)\n",
    "                    print(driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div[1]/div/div[1]/div[4]').text)\n",
    "                    rev_text = driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div[1]/div/div[1]/div[4]').text\n",
    "                else:\n",
    "                    if (driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div['+str(i)+']/div/div[1]/div[3]').text == ''):\n",
    "                        print('***********')\n",
    "                        if(driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div['+str(i)+']/div/div[1]/div[4]/div/a').text == 'read more'):\n",
    "                            print('****************')\n",
    "                            url = '//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div['+str(i)+']/div/div[1]/div[4]/div/a'\n",
    "                            driver.find_element_by_xpath(url).click()\n",
    "                            time.sleep(5)\n",
    "                    print(driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div['+str(i)+']/div/div[1]/div[3]').text)\n",
    "                    rev_text = driver.find_element_by_xpath('//*[@id=\"reviews-container\"]/div[1]/div[3]/div/div/div[1]/div['+str(i)+']/div/div[1]/div[3]').text\n",
    "            except:\n",
    "                rev_text=''\n",
    "            # check the number of followers and number of reviews reviewers\n",
    "            # There are some cases where user might have no followers, in that case there will be only number of reviews option\n",
    "            # Hence to consider followers and number of reviewers separately\n",
    "            try:\n",
    "                rev_followers = review.find_element_by_xpath('.//span[@class = \"grey-text fontsize5 nowrap\"]').text\n",
    "                rev_followers = rev_followers.split()\n",
    "                rev_reviews  = int(rev_followers[0])\n",
    "            except:\n",
    "                rev_reviews = 0\n",
    "            print(rev_reviews)\n",
    "            try:\n",
    "                rev_followers = int(rev_followers[3])\n",
    "            except:\n",
    "                rev_followers = 0\n",
    "            print(rev_followers)\n",
    "            try:\n",
    "                review_photo = len(review.find_elements_by_xpath('.//div[@class=\"ui image js-heart-container\"]'))\n",
    "            except:\n",
    "                review_photo = 0\n",
    "            try:\n",
    "                additional_photo = int(str(review.find_element_by_xpath('.//div[@class=\"overlay res-photo-thumbnail tac\"]').text).split()[0][1:])\n",
    "            except:\n",
    "                additional_photo = 0\n",
    "            rev_photos = review_photo+additional_photo\n",
    "            print(rev_photos)\n",
    "            try:\n",
    "                print(review.find_element_by_xpath('.//time').get_attribute('datetime'))\n",
    "                rev_datetime = review.find_element_by_xpath('.//time').get_attribute('datetime')\n",
    "            except:\n",
    "                rev_datetime=''\n",
    "            #Saving the review information in dictionary    \n",
    "            dict1['Reviewer_Name'] = rev_name\n",
    "            dict1['Review_text'] = rev_text\n",
    "            dict1['Review_time'] = rev_datetime\n",
    "            dict1['NoOfReview'] = rev_reviews\n",
    "            dict1['NoOfFollower'] = rev_followers\n",
    "            dict1['NoOfPhoto'] = rev_photos\n",
    "            #writing dictionary to the CSV file\n",
    "            writer.writerow(dict1.values())\n",
    "            print('**********************************')\n",
    "        except:\n",
    "            continue\n",
    "    print('-------completed-----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the review function for first 100 restaurants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(0,101):\n",
    "    url=df_restaurant['Link'][j]\n",
    "    print(url)\n",
    "    GetReviews(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Close the CSV file and Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file.close()    \n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
